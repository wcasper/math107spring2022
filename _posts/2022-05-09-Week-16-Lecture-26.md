---
layout: post
---

Notes and highlights from lecture

## Today's Objectives

* Decomposing matrices
* Singular value decomposition

## Factoring matrices

In applications, many numerical methods are based on strategic decomposition of matrices as a product of other matrices with certain nice qualities.
Some important famous examples include
* Diagonalization
* Singular value decomposition
* LU decomposition
* QR decomposition
* Cholesky decomposition

Depending on the application in mind, one factorization may be more desirable than another.
We will focus here on some factorizations associated with the eigenvectors and eigenvalues of a matrix.


## Diagonalization

Let $$A$$ be an $$n\times n$$ matrix.  A **diagonalization** of $$A$$ is a way of relating the matrix $$A$$ to a diagonal matrix.

**Definition:** An $$m\times n$$ matrix $$D$$ is called **diagonal** if all of the entries away from the main diagonal are zero, ie.

$$D(j,k) = 0\ \text{for all j\neq k}.$$

A **diagonalization** of $$A$$ is a pair of matrices $$P$$ and $$D$$ with $$P$$ an invertible matrix and $$D$$ a diagonal matrix satisfying

$$A = PDP^{-1}.$$

### Calculating a diagonalization

Let $$A$$ be an $$n\times n$$ matrix.
To calculate the diagonalization of $$A$$ we use the following steps.

**Diagonalization Steps**

* (1) Calculate the $$n$$ eigenvalues $$\lambda_1,\dots\lambda_n$$ of $$A$$ (counting multiplicity)
* (2) For each eigenvalue $$\lambda_k$$, calculate an eigenvector of $$\vec v_k$$ of $$A$$ with eigenvalue $$\lambda$$.
* (3) Let $$P$$ be the matrix whose columns are the eigenvectors we found and $$D$$ the diagonal matrix of eigenvalues.

$$P = [\vec v_1\ \vec v_2\ \dots\ \vec v_n],\quad D = \text{diag}(\lambda_1,\lambda_2,\dots,\lambda_n).$$

If $$P$$ is invertible, then $$P$$ and $$D$$ define a diagonalization of $$A$$.

In MATLAB, these steps are all automatically accomplished using the *eig* command:

```Matlab
[P,D] = eig(A);
```

Note that in the algorithm described above, we required that the matrix $$P$$ be invertible.  It can happen that no matter what eigenvectors we choose the matrix $$P$$ will not be invertible.  In such a case, a diagonalization of a matrix does not always exist.  Matrices which do not have a diagonalization are called **degenerate**.  Matrices that do have a diagonalization are called **nondegenerate**.

### Diagonalization of symmetric matrices
An important special case of matrices in the context of diagonalization are symmetric matrices.

**Definition:** A real, $$n\times n$$ matrix $$A$$ is called **symmetric** if it is equal to its transpose, ie $$A^T=A$$.

A symmetric matrix is always diagonalizable by a very special matrix called an orthogonal matrix.

**Definition:** A real, $$n\times n$$ matrix $$U$$ is called **orthogonal** or **unitary** if $$U^T=U^{-1}$$.

**Theorem:** Let $$A$$ be a real, symmetric $$n\times n$$ matrix.  Then there exists an orthogonal matrix $$U$$ and a real diagonal matrix $$D$$ satisfying

$$A = PDP^{-1}.$$

In MATLAB, if $$A$$ is a real, symmetric matrix then the *eig* command automatically returns an orthogonal matrix.

### Applications of diagonalization

Diagonalization has a lot of useful properties for matrices, for example in calculating large powers of matrices.
The key observation is that powers behave well with respect to diagonal decompositions.  For example

$$(PDP^{-1})^2 = PDP^{-1}PDP^{-1} = PDIDP^{-1} = PD^2P^{-1}$$

$$(PDP^{-1})^3 = (PDP^{-1})^2PDP^{-1} = PD^2P^{-1}PDP^{-1} = PD^2IDP^{-1} = PD^3P^{-1}$$

In general:

$$(PDP^{-1})^n = PD^nP^{-1},\ \text{for all n>0}.$$

Suppose, for example, we want to calculate  $$A^{100}$$ for

$$A = \left(\begin{array}{cc}1 & 1\\ 0 & 2\end{array}\right).$$

The eigenvalues of $$A$$ are $$1$$ and $$2$$.  The vector $$\binom{1}{0}$$ is an eigenvector with eigenvalue $$1$$ and the vector $$\binom{1}{1}$$ is an eigenvector with eigenvalue $$2$$.  Thus if we take

$$
P = \left(\begin{array}{cc}1 & 1\\ 0 & 1\end{array}\right),\quad
D = \left(\begin{array}{cc}1 & 0\\ 0 & 2\end{array}\right)
$$

then we have $$A = PDP^{-1}.$$  Now if want to calculate $$A^{100}$$, then we simply find

$$
\begin{align*}
A^{100}
  & = (PDP^{-1})^{100} = PD^{100}P^{-1}\\
  & = \left(\begin{array}{cc}1 & 1\\ 0 & 1\end{array}\right)
      \left(\begin{array}{cc}1 & 0\\ 0 & 2^{100}\end{array}\right)
      \left(\begin{array}{cc}1 & -1\\ 0 & 1\end{array}\right)\\
  & = \left(\begin{array}{cc}1 & 1\\ 0 & 1\end{array}\right)
      \left(\begin{array}{cc}1 & -1\\ 0 & 2^{100}\end{array}\right)\\
  & = \left(\begin{array}{cc}1 & 2^{100}-1\\ 0 & 2^{100}\end{array}\right).
\end{align*}
$$


### Fibonacci sequences

The Fibonacci sequence is the sequence $$f_1,f_2,f_3,\dots$$ defined recursively by

$$f_1=1,\ f_2=1,\ f_3=2,\ \dots f_{n+1} = f_{n} + f_{n-1}\ \forall n\geq 2.$$


Thus the first several terms of the Fibonacci sequence are

| $$n$$ | $$f_n$$ |
| ----- | ------- |
|   1   |    1    |
|   2   |    1    |
|   3   |    2    |
|   4   |    3    |
|   5   |    5    |
|   6   |    8    |
|   7   |   13    |
|   8   |   21    |
|   9   |   34    |
|  10   |   55    |

We can use diagonalization to get an explicit formula for the $$n$$'th term $$f_n$$ of the Fibonacci sequence.
We start by noticing that the $$2\times 2$$ matrix

$$A = \left(\begin{array}{cc}0 & 1\\ 1 & 1\end{array}\right)$$

satisfies

$$A\binom{f_{n-1}}{f_n} = \binom{f_n}{f_{n+1}}.$$

Thus for example

$$A\binom{1}{1} = \frac{1}{2}$$

$$A^2\binom{1}{1} = A\binom{1}{2} = \frac{2}{3}$$

$$A^3\binom{1}{1} = A\binom{2}{3} = \frac{3}{5}$$

In general

$$A^n\binom{1}{1} = \binom{f_{n+1}}{f_{n+2}}.$$

Thus to get a closed form solution, we should perform the diagonalization of $$A$$.

The eigenvalues of $$A$$ are roots of the characteristic polynomial polynomial $$x^2-x-1$$, which are $$\gamma_{\pm} = (1\pm\sqrt{5})/2$$.
The value $$\gamma_+$$ is particularly special as it is the well-known **golden ratio**.

An eigenvector with eigenvalue $$\gamma_\pm$$ is $$\binom{\gamma_\pm-1}{1}$$.
Thus if we take

$$
P = \left(\begin{array}{cc}\gamma_+-1 & \gamma_--1\\ 1 & 1\end{array}\right),\quad
D = \left(\begin{array}{cc}\gamma_+ & 0\\ 0 & \gamma_-\end{array}\right)
$$

then we have $$A = PDP^{-1}.$$  Therefore

$$A^n = PD^nP^{-1}$$

Putting this together with our formula above, we find

$$PD^nP^{-1}\binom{1}{1} = \binom{f_{n+1}}{f_{n+2}}.$$

Working out the terms explicitly,

we get

$$f_{n+2} = \frac{2\gamma_+^n-2\gamma_-^n - \gamma_+^n\gamma_- + \gamma_-^n\gamma^+}{\gamma_+-\gamma_-}.$$

For example, for $$n=98$$ this gives the hundredth term in the Fibonacci sequence:

$$f_{100} = 354224848179261915075.$$

## Singular Value Decomposition

Principal component analysis is very related to another idea, called **singular value decomposition**, or SVD.
In SVD, an $$m\times n$$ matrix $$A$$ is written as a product

$$A = UDV^T$$

for $$D$$ an $$m\times n$$ diagonal matrix, $$U$$ an $$m\times m$$ matrix and $$V$$ an $$n\times n$$ matrix satisfying $$U^T = U^{-1}$$ and $$V^T=V^{-1}$$.

Note that the matrices $$U$$ and $$V$$ are called unitary.

**Definition:** A matrix $$U$$ is called unitary if $$U^T=U^{-1}$$.

The process of calculating the SVD is as follows

**SVD Algorithm Steps**

* (1) Find an $$n\times n$$ unitary matrix $$V$$ whose columns are eigenvectors of $$A^TA$$.  In MATLAB this can be done by

```Matlab
[V,F] = eig(transpose(A)*A);
```

* (2) Find an $$m\times m$$ unitary matrix $$U$$ whose columns are eigenvectors of $$AA^T$$.  In MATLAB this can be done by

```Matlab
[U,E] = eig(A*transpose(A));
```

* (3) Set $$D = U'*A*V$$, which should be an $$m\times n$$ diagonal matrix whose entries are the eigenvalues we calculated above


PCA can be viewed as equivalent to SVD when things are examined carefully.
Moreover, SVD has the direct application of allowing for low rank approximations of a specific matrix $$A$$ by simply making some of the entries of $$D$$ equal to zero.
This has many applications, such as image compression.

## Reading assignments

* <a target="_parent" href="../../../extras/textbook.pdf">primary text (link)</a>: chapter 12 Section 3
* When Life Is Linear: Ch. 7 (14 pages)

## Additional resources

**Lecture notes:** <a target="_parent" href="https://wcasper.github.io/math107spring2021/extras/notes/2021-05-05-Note-09-48.pdf">notes for lecture (link)</a>

**Lecture video:** <a target="_parent" href="https://fullerton.zoom.us/rec/share/3Ov68B1TqqxolqFGEsEJcDEqNYh2sYLJ4KF1bOUjfPt1UnZBqiUlI2p1SsFRZ_4l.hpWAwo3WJrd-wjqw">recording of lecture (link)</a>

The passcode can be found on the <a target="_parent" href="https://csufullerton.instructure.com/courses/3127326/pages/video-lecture-keys">passcode page (link)</a>



